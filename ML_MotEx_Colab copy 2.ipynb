{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inAkwrhMtEfJ"
   },
   "source": [
    "<h1><center> Welcome to ML-MotEx </center></h1>\n",
    "\n",
    "**Github:** https://github.com/AndySAnker/ML-MotEx\n",
    "\n",
    "**Paper:** Extracting Structural Motifs from Pair Distribution Function Data of Nanostructures using Explainable Machine Learning https://www.nature.com/articles/s41524-022-00896-3\n",
    "\n",
    "**Questions:** andy@chem.ku.dk\n",
    "\n",
    "Use this script to use ML-MotEx to extract a structural motif from a dataset. This script does only handle Pair Distribution Function (PDF) data but ML-MotEx can in principle handle any data.\n",
    "\n",
    "If you have already fitted the dataset with multiple structures or you have have another type of data than PDF please go on to step 3 halfway through the script.\n",
    "\n",
    "# First install python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_w_GJCPBzOoA"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\n",
    "!sudo apt-get update\n",
    "!sudo apt-get -y install gcc-4.9\n",
    "!sudo apt-get upgrade libstdc++6\n",
    "\n",
    "!pip install ase shap xgboost bayesian-optimization\n",
    "!git clone https://github.com/AndySAnker/ML-MotEx.git\n",
    "\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_QNc_Z7tdaU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash \n",
    "MINICONDA_INSTALLER_SCRIPT=Miniconda3-latest-Linux-x86_64.sh\n",
    "MINICONDA_PREFIX=/usr/local\n",
    "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
    "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
    "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuVXdl3ktfdC"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!conda create -n diffpy -c defaults -c diffpy python=3.7 diffpy-cmi pandas --yes\n",
    "!cp -r /usr/local/envs/diffpy/lib/python3.7/site-packages/diffpy.srfit-3.0.0-py3.7.egg/diffpy/* /usr/local/envs/diffpy/lib/python3.7/site-packages/diffpy/\n",
    "!cp -r /usr/local/envs/diffpy/lib/python3.7/site-packages/diffpy.structure-3.0.1-py3.7.egg/diffpy/* /usr/local/envs/diffpy/lib/python3.7/site-packages/diffpy/\n",
    "!cp -r /usr/local/envs/diffpy/lib/python3.7/site-packages/diffpy.utils-3.0.0-py3.7.egg/diffpy/* /usr/local/envs/diffpy/lib/python3.7/site-packages/diffpy/\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"/usr/local/envs/diffpy/lib/python3.7/site-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBZxEr5ctgFI"
   },
   "source": [
    "# import modules, set seed parameters and import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-NU9B66atC-R"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm\n",
    "import matplotlib.pyplot as plt\n",
    "import time, random\n",
    "import shap\n",
    "from scipy.optimize.minpack import leastsq\n",
    "from diffpy.Structure import Structure, Atom\n",
    "from diffpy.srfit.pdf import PDFContribution, PDFParser, PDFGenerator\n",
    "from diffpy.srfit.fitbase import FitRecipe, FitResults, Profile, FitContribution\n",
    "from diffpy.srreal.pdfcalculator import DebyePDFCalculator\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from ase.io import read\n",
    "#from google.colab import output, files\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "random.seed(14)\n",
    "np.random.seed(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "CIM9y9ECtRdz"
   },
   "outputs": [],
   "source": [
    "def Load_starting_model(starting_model):\n",
    "    \"\"\"This function loads the structure\"\"\"\n",
    "\n",
    "    # Read structure and divide it into two lists: Atoms we want to iterate (W) and atoms we do not iterate (O)\n",
    "    struct=[]\n",
    "    with open(starting_model, 'r') as fi:\n",
    "        for line in fi.readlines():\n",
    "            sep_line=line.strip('{}\\n\\r ').split()\n",
    "            if len(sep_line)==4: #  tillader andre informationer i xyz filen some ikke skal laeses\n",
    "                struct.append(sep_line)\n",
    "    elements=np.array(struct)[:,0]\n",
    "    xyz=(np.array(struct)[:,1:].astype(float))\n",
    "    \n",
    "    return elements, xyz\n",
    "\n",
    "def format_XYZ(starting_model, allowed_atoms):\n",
    "\t# Read structure and divide it into two lists: Atoms we want to iterate and atoms we do not iterate.\n",
    "\t# Save the file in this new format and get number of atoms that we iterate.\n",
    "\tpermutable_struct = []\n",
    "\tnonpermutable_struct = []\n",
    "\twith open(starting_model, 'r') as fi:\n",
    "\t\tfor line in fi.readlines():\n",
    "\t\t\tsep_line=line.strip('{}\\n\\r ').split()\n",
    "\t\t\tif len(sep_line)==4: #  tillader andre informationer i xyz filen some ikke skal laeses\n",
    "\t\t\t\tif sep_line[0] in allowed_atoms:\n",
    "\t\t\t\t\tpermutable_struct.append(sep_line)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnonpermutable_struct.append(sep_line)\n",
    "\tstruct = permutable_struct + nonpermutable_struct\n",
    "\n",
    "\tNew_XYZ = open(starting_model, \"w\")\n",
    "\tNew_XYZ.write(str(len(struct)) + \"\\n\\n\")\n",
    "\tfor i in range(len(struct)):\n",
    "\t\tNew_XYZ.write(str(struct[i]).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\",\", \"\") + \"\\n\")\n",
    "\tNew_XYZ.close()\n",
    "\n",
    "\tNum_permutable_atoms = len(permutable_struct)\n",
    "\treturn Num_permutable_atoms\n",
    "  \n",
    "def structure_catalogue_maker(Number_of_structures, Number_of_atoms, lower_atom_number, higher_atom_number):\n",
    "    \"\"\"This function makes a shuffled list containing 'Number_of_structures' number of lists which each is \n",
    "    'Number_of_atoms' long and is randomly distributed with 0's and 1's whereas the minimum number of 1's are \n",
    "    'lower_atom_number' and the maximum number of 1's are 'higher_atom_number'.\"\"\"\n",
    "    \n",
    "    print (\"Starting to make a structure catalogue with: \", str(Number_of_structures) + \" structure from the starting model.\")\n",
    "    print (\"The structure will have between \" + str(lower_atom_number) + \" and \" + str(higher_atom_number) + \" atoms\")\n",
    "    structure_catalogue = []\n",
    "    for i in range(Number_of_structures):\n",
    "        one_count = random.randint(lower_atom_number, higher_atom_number)\n",
    "        zero_count = Number_of_atoms  - one_count\n",
    "        my_list = [0]*zero_count + [1]*one_count\n",
    "        random.shuffle(my_list)\n",
    "        my_list.insert(0, one_count)\n",
    "        structure_catalogue.append(my_list)\n",
    "    print (\"Permutations Succeeded\")\n",
    "    return structure_catalogue\n",
    "    \n",
    "def fitting(structure_catalogue, atom_ph, Qmin, Qmax, Qdamp, rmin, rmax, plot, index):\n",
    "    \"\"\"This function takes in a 'starting_model', and an 'index' from the 'structure_catalogue'. It generates the \n",
    "    corresponding structure and fit it to the 'Experimental_Data'.\"\"\"\n",
    "    \n",
    "    # Read structure and divide it into two lists: Atoms we want to iterate (W) and atoms we do not iterate (O)\n",
    "    stru = read(starting_model)\n",
    "    xyz = stru.get_positions()\n",
    "    xyz_W = xyz[:NumW].copy()\n",
    "    xyz_O = xyz[NumW:len(xyz)].copy()\n",
    "    keep_O = np.zeros(len(xyz_O))\n",
    "    h = 0\n",
    "    # Cycle through W atoms and delete W according to index 0's from permutation\n",
    "    permutations = np.asarray(structure_catalogue)[:,1:]\n",
    "    for j in range(len(xyz_W)):\n",
    "        if permutations[index][j] == 0:\n",
    "            xyz_W = np.delete(xyz_W,j - h,0)\n",
    "            h = h+1   \n",
    "    # Cycle through all atoms that is not iteratable and test if it is within the threshold distance. Delete atoms with no bonds\n",
    "    for j in range(len(xyz_O)):        \n",
    "        for k in range(len(xyz_W)):\n",
    "            dist = np.linalg.norm(xyz_W[k] - xyz_O[j])\n",
    "            if dist < threshold:    \n",
    "                keep_O[j] = 1\n",
    "                break\n",
    "    h = 0            \n",
    "    for j in range(len(xyz_O)):\n",
    "        if keep_O[j] == 0:\n",
    "            xyz_O = np.delete(xyz_O,j - h, 0)\n",
    "            h += 1\n",
    "            \n",
    "    # Create structure for iterable (W) and non-iterable (O) atoms and combine them\n",
    "    W_cluster = Structure([Atom('Th', xi) for xi in xyz_W])\n",
    "    O_cluster = Structure([Atom('O', xi) for xi in xyz_O])\n",
    "    cluster = W_cluster + O_cluster\n",
    "    \n",
    "    # Make a standard cluster refinement using Diffpy-CMI\n",
    "    # Import the data and make it a PDFprofile. Define the range of the data that will be used in the fit.\n",
    "    pdfprofile = Profile()\n",
    "    pdfparser = PDFParser()\n",
    "    pdfparser.parseFile(Experimental_Data)\n",
    "    pdfprofile.loadParsedData(pdfparser)\n",
    "    pdfprofile.setCalculationRange(xmin = rmin, xmax = rmax)\n",
    "\n",
    "    # Setup the PDFgenerator that calculates the PDF from the structure\n",
    "    pdfgenerator_cluster = PDFGenerator(\"G\")\n",
    "    # Add the profile and both generators to the PDFcontribution\n",
    "    pdfcontribution = FitContribution(\"pdf\")\n",
    "    pdfcontribution.setProfile(pdfprofile, xname=\"r\") \n",
    "    pdfcontribution.addProfileGenerator(pdfgenerator_cluster)\n",
    "    \n",
    "    pdfgenerator_cluster.setQmin(Qmin)\n",
    "    pdfgenerator_cluster.setQmax(Qmax)\n",
    "    pdfgenerator_cluster._calc.evaluatortype = 'OPTIMIZED'\n",
    "    pdfgenerator_cluster.setStructure(cluster, periodic = False)\n",
    "\n",
    "    # Use scaling factors proportional to molar content\n",
    "    pdfcontribution.setEquation('mc*G')\n",
    "\n",
    "    # Define the recipe to do the fit and add it to the PDFcontribution\n",
    "    recipe = FitRecipe()\n",
    "    recipe.addContribution(pdfcontribution)\n",
    "\n",
    "    # Avoid too much output during fitting \n",
    "    recipe.clearFitHooks()\n",
    "\n",
    "    # Add the scale factor.\n",
    "    recipe.addVar(pdfcontribution.mc, 1.0, tag = \"scale\", fixed=False)\n",
    "    \n",
    "    # Add the instrumental parameters to the two generators\n",
    "    pdfgenerator_cluster.qdamp.value = Qdamp\n",
    "    \n",
    "    # Add the delta2 parameters, and make sure it cannot take unphysical values\n",
    "    recipe.addVar(pdfgenerator_cluster.delta2, 0, name = \"delta2_cluster\", tag = \"delta2\")\n",
    "\n",
    "    # Add ADP and \"cell\" for the cluster\n",
    "    phase_cluster = pdfgenerator_cluster.phase\n",
    "    atoms = phase_cluster.getScatterers()\n",
    "    lat = phase_cluster.getLattice()\n",
    "\n",
    "    recipe.newVar(\"zoomscale1\", 1.0, tag = \"lat\")\n",
    "    recipe.newVar(\"zoomscale2\", 1.0, tag = \"lat\")\n",
    "    recipe.newVar(\"zoomscale3\", 1.0, tag = \"lat\")\n",
    "    recipe.constrain(lat.a, 'zoomscale1')\n",
    "    recipe.constrain(lat.b, 'zoomscale2')\n",
    "    recipe.constrain(lat.c, 'zoomscale3')\n",
    "\n",
    "    W_cluster = recipe.newVar(\"W_Biso_cluster1\", 1, tag = 'adp_w')\n",
    "    O_cluster = recipe.newVar(\"O_Biso_cluster1\", 5, tag = 'adp_o')\n",
    "\n",
    "    for atom in atoms:\n",
    "      if atom.element.title() == atom_ph:\n",
    "            recipe.constrain(atom.Biso, W_cluster)\n",
    "      elif atom.element.title() == \"O\":\n",
    "            recipe.constrain(atom.Biso, O_cluster)\n",
    "\n",
    "    recipe.restrain(\"zoomscale1\", lb = 0.99, ub = 1.01, sig = 0.001)\n",
    "    recipe.restrain(\"zoomscale2\", lb = 0.99, ub = 1.01, sig = 0.001)\n",
    "    recipe.restrain(\"zoomscale3\", lb = 0.99, ub = 1.01, sig = 0.001)\n",
    "    \n",
    "    #free parameters are set\n",
    "    recipe.fix('all')\n",
    "    recipe.free(\"scale\", \"lat\")\n",
    "\n",
    "    # Turn off printout of iteration number.\n",
    "    #recipe.clearFitHooks()\n",
    "\n",
    "    # We can now execute the fit using scipy's least square optimizer.\n",
    "    leastsq(recipe.residual, recipe.getValues())\n",
    "    \n",
    "    # We calculate the goodness-of-fit, Rwp\n",
    "    g = recipe.pdf.profile.y\n",
    "    gcalc = recipe.pdf.evaluate()\n",
    "    rfactor1 = np.sqrt(sum((g - gcalc)**2) / sum((g)**2))\n",
    "    \n",
    "    # if plot == 1 it will also plot the fit\n",
    "    if plot == 1:\n",
    "        print (\"FIT RESULTS\\n\")\n",
    "        res1 = FitResults(recipe)\n",
    "        print (res1)\n",
    "\n",
    "        # Plot the observed and refined PDF.\n",
    "        # Get the experimental data from the recipe\n",
    "        r = recipe.pdf.profile.x\n",
    "        gobs = recipe.pdf.profile.y\n",
    "\n",
    "        # Get the calculated PDF and compute the difference between the calculated and measured PDF\n",
    "        gcalc = recipe.pdf.evaluate()\n",
    "        baseline = 1.1 * gobs.min()\n",
    "        gdiff = gobs - gcalc\n",
    "\n",
    "        # Plot!\n",
    "        plt.figure()\n",
    "        plt.plot(r, gobs, 'bo', label=\"G(r) data\")\n",
    "        plt.plot(r, gcalc, 'r-', label=\"G(r) fit\")\n",
    "        plt.plot(r, gdiff + baseline, 'g-', label=\"G(r) diff\")\n",
    "        plt.plot(r, np.zeros_like(r) + baseline, 'k:')\n",
    "        plt.xlabel(r\"$r (\\AA)$\")\n",
    "        plt.ylabel(r\"$G (\\AA^{-2})$\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "    return rfactor1\n",
    "\n",
    "def fitting_multiprocess(structure_catalogue, atom_ph, Qmin, Qmax, Qdamp, rmin, rmax, SaveName, cores=1):\n",
    "    \"\"\"This function runs the refinement of all the structures in the structure catalogue using multiprocessing\"\"\"\n",
    "    start_time = time.time()\n",
    "    values = []\n",
    "    # Set up multiprocessing refinement\n",
    "    fitindex = range(len(structure_catalogue))\n",
    "    p = Pool(processes=cores)\n",
    "    plot = 0\n",
    "    func = partial(fitting, structure_catalogue, atom_ph, Qmin, Qmax, Qdamp, rmin, rmax, plot)\n",
    "    results = p.map(func, fitindex)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    \n",
    "    # Start refinement and append results to lists\n",
    "    for i in fitindex:\n",
    "        if i % 100 == 0:\n",
    "            print (\"I have now fitted: \", str(i) + \" structures out of \" + str(len(structure_catalogue)))\n",
    "        rw = results[i]\n",
    "        values.append(i)\n",
    "        values.append(rw)\n",
    "    values = np.reshape(values,(int(len(values)/2) , 2))\n",
    "    \n",
    "    # Save results in format that is suitable for Machine Learning\n",
    "    print (\"Best fit\")\n",
    "    print (values[np.argmin(values[:,1])])\n",
    "    print(\"Total execution time: %.3fs\" % (time.time()-start_time))\n",
    "    Result = np.column_stack([values, np.asarray(structure_catalogue)[values[:,0].astype(int)]])\n",
    "    np.savetxt(SaveName, Result)\n",
    "    return Result\n",
    "\n",
    "def Import_Dataset(FileName):\n",
    "    \"\"\"This function loads a catalogue of structures with their corresponding Rwp values and split the dataset \n",
    "    into a training set and validation set with features and labels.\"\"\"\n",
    "    # load data\n",
    "    dataset = np.loadtxt(FileName, delimiter=\" \", skiprows=0)\n",
    "    dataset_original = dataset.copy()\n",
    "\n",
    "    # Split into training and validation set\n",
    "    dataset_train = dataset[:int(len(dataset)*0.8)]\n",
    "    dataset_val = dataset[int(len(dataset)*0.8):len(dataset)]\n",
    "    \n",
    "    # split data into features (X) and labels (y)\n",
    "    X_train = dataset_train[:,2:len(dataset)+1]\n",
    "    y_train = dataset_train[:,1]\n",
    "    X_val = dataset_val[:,2:len(dataset)+1]\n",
    "    y_val = dataset_val[:,1]\n",
    "    \n",
    "    print(\"Number of Training Data:\", len(y_train))\n",
    "    print(\"Number of Validation Data:\", len(y_val))\n",
    "        \n",
    "    return X_train, y_train, X_val, y_val\n",
    "    \n",
    "def Validate_XGBoost(model, X_val, y_val):\n",
    "    \"\"\"Function to validate the performance of the XGBoost algorithm on a validation set\"\"\"\n",
    "    print (\"Giving an estimate of the accuracy of the model\")\n",
    "    xgb_val = xgb.DMatrix(X_val, y_val)\n",
    "    y_pred_val = model.predict(xgb_val)\n",
    "    rmse = mean_squared_error(y_val, y_pred_val)\n",
    "    print(\"RMSE: %f\" % (rmse))\n",
    "    return rmse\n",
    "\n",
    "def DecisionTree_CrossValidation(learning_rate, max_depth, data, targets):\n",
    "    \"\"\"Decision Tree cross validation.\n",
    "       Fits a Decision Tree with the given paramaters to the target \n",
    "       given data, calculated a CV accuracy score and returns the mean.\n",
    "       The goal is to find combinations of max_depth, min_samples_leaf \n",
    "       that maximize the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    estimator = xgb.XGBRegressor(random_state=0, \n",
    "                                       learning_rate = learning_rate,\n",
    "                                       max_depth=max_depth,\n",
    "                                       objective='reg:squarederror') \n",
    "    \n",
    "    cval = cross_val_score(estimator, data, targets, cv=3)\n",
    "    return cval.mean()\n",
    "\n",
    "def optimize_DecisionTree(data, targets, pars, n_iter=5):\n",
    "    \"\"\"Apply Bayesian Optimization to Decision Tree parameters.\"\"\"\n",
    "    \n",
    "    def crossval_wrapper(learning_rate, max_depth):\n",
    "        \"\"\"Wrapper of Decision Tree cross validation. \n",
    "           Notice how we ensure max_depth, min_samples_leaf \n",
    "           are casted to integer before we pass them along.\n",
    "        \"\"\"\n",
    "        accuracy_mean = DecisionTree_CrossValidation(learning_rate = learning_rate,\n",
    "                                       max_depth=int(max_depth), \n",
    "                                       data=data, targets=targets)\n",
    "        \n",
    "        return accuracy_mean\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=crossval_wrapper, pbounds=pars, \n",
    "                                     random_state=0, verbose=2)\n",
    "    optimizer.maximize(init_points=4, n_iter=n_iter)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "def train_w_earlyStop(X_train, y_train, learning_rate, max_depth, n_estimators, n_jobs, gamma, min_child_weight, base_score, seed, early_stop, xgb_model=None):\n",
    "    \"\"\"Train a XGBoost model using the given parameters. The training will run until the early stop criteria is\n",
    "    fulfilled or 5000 epochs are run. The loss curve and values is saved.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    xgb_params = {}\n",
    "    xgb_params['learning_rate'] = learning_rate\n",
    "    xgb_params['objective'] = 'reg:squarederror' # Default\n",
    "    xgb_params['max_depth'] = max_depth\n",
    "    xgb_params['n_estimators'] = n_estimators\n",
    "    xgb_params['n_jobs'] = n_jobs\n",
    "    xgb_params['gamma'] = gamma\n",
    "    xgb_params['min_child_weight'] = min_child_weight\n",
    "    xgb_params['eval_metric'] = ['mae']\n",
    "    xgb_params['base_score'] = base_score\n",
    "    xgb_params['seed'] = seed\n",
    "    xgb_params['verbosity'] = 0\n",
    "    epochs = 5000\n",
    "\n",
    "    store = {}\n",
    "    xgb_train = xgb.DMatrix(X_train, y_train)\n",
    "    xgb_val = xgb.DMatrix(X_val, y_val)\n",
    "    evallist = [(xgb_train,'train'),(xgb_val, 'val')]\n",
    "    model = None\n",
    "\n",
    "    model = xgb.train(xgb_params, xgb_train, epochs, evallist, evals_result=store, verbose_eval=0, early_stopping_rounds=early_stop, xgb_model=model)\n",
    "    print (\"Training using the best parameters\")\n",
    "    print(\"Total execution time: %.3f s\" % (time.time()-start_time))\n",
    "    print (\"Training succeeded\")\n",
    "    \n",
    "    # Save Loss\n",
    "    loss_results = pd.DataFrame(store)\n",
    "    loss_results.to_csv(\"LossCurve.csv\")\n",
    "\n",
    "    # Plot Loss\n",
    "    test_score = store['val']['mae']\n",
    "    train_score = store['train']['mae']\n",
    "    plt.plot(range(len(test_score)), test_score, \"c\", label=\"Val\")\n",
    "    plt.plot(range(len(train_score)), train_score, \"orange\", label=\"Train\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MAE Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"LossCurve.png\")\n",
    "    return model, store\n",
    "\n",
    "def shap_essential_figure(model, X_train, saveResults):\n",
    "    \"\"\"Function that takes a XGBoost model, a training set and calculates SHAP values of each features in the \n",
    "    training set. Afterwards, it saves a SHAP summary plot\"\"\"\n",
    "    # Calculate SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_train) \n",
    "    # Make SHAP summary plot\n",
    "    shap.summary_plot(shap_values[:,1:], X_train[:,1:], feature_names=[\"Atom #\"+str(i) for i in range(1,X_train.shape[1])], color_bar_label=\"Feature Value (Low: Atom Removed, High: Atom Not Removed)\", show=False, max_display=X_train.shape[1]) # to plot these explanations\n",
    "    plt.savefig(saveResults + \"SHAP_values.png\", dpi=600, format = \"png\", bbox_inches='tight')\n",
    "    return explainer, shap_values\n",
    "\n",
    "def calculate_atomContributionValue(shap_values, X_train, saveResults):\n",
    "    \"\"\"Calculate atom contribution value list from the result array\"\"\"\n",
    "    \n",
    "    # Define AtomContributionValues vector\n",
    "    AtomContributionValues = []\n",
    "    AtomContributionValues_RMS = []\n",
    "    for i in range(X_train.shape[1]):\n",
    "        # We are not interested in the number of atoms in this regi\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else:\n",
    "            Keep_atoms = np.mean((shap_values[np.where(X_train[:,i] == 1),i]))\n",
    "            Keep_atoms_RMS = np.std((shap_values[np.where(X_train[:,i] == 1),i]))\n",
    "            Remove_atoms = np.mean((shap_values[np.where(X_train[:,i] == 0),i]))\n",
    "            Remove_atoms_RMS = np.std((shap_values[np.where(X_train[:,i] == 0),i]))\n",
    "            AtomContributionValues.append(0.5*Keep_atoms - 0.5*Remove_atoms)\n",
    "            AtomContributionValues_RMS.append(np.sqrt(0.5*Keep_atoms_RMS**2 + 0.5*Remove_atoms_RMS**2))\n",
    "\n",
    "    for i in range(len(AtomContributionValues_RMS)):\n",
    "      print (\"Atom\", i+1,\" AtomContributionValue: \", AtomContributionValues[i], \" +/- \", AtomContributionValues_RMS[i], \"Atom\", i+1,\" confidence factor: \", abs(AtomContributionValues[i]/AtomContributionValues_RMS[i]))\n",
    "    print (\"Overall confidence factor: \", np.nanmean(np.abs(np.array(AtomContributionValues)/np.array(AtomContributionValues_RMS))), \" +/- \", np.nanstd(np.abs(np.array(AtomContributionValues)/np.array(AtomContributionValues_RMS))))\n",
    "\n",
    "    # Normalise the AtomContributionValues\n",
    "    amin, amax = min(AtomContributionValues), max(AtomContributionValues)\n",
    "    AtomContributionValues = (AtomContributionValues - amin) / (amax - amin)\n",
    "    AtomContributionValues_ph = AtomContributionValues.copy()\n",
    "    AtomContributionValues_ph.sort()\n",
    "\n",
    "    # Define colormap of viridis.reverse\n",
    "    norm = mpl.colors.Normalize(vmin=AtomContributionValues_ph[round((len(AtomContributionValues))/10)], vmax=AtomContributionValues_ph[-round((len(AtomContributionValues))/10)])\n",
    "    cmap = matplotlib.cm.cividis_r\n",
    "    m = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    \n",
    "    # Save results to file\n",
    "    f = open(saveResults+\"AtomContributionValues.txt\", \"w\")\n",
    "    f.write(\"\\nAtom contribution are calculated to: \\n\")\n",
    "    for i in range(len(AtomContributionValues)):\n",
    "        f.write(\"Atom # \"+ str(i+1) + \":  \"+ str(AtomContributionValues[i]) + \"  Colorcode:  \"+ mpl.colors.rgb2hex(m.to_rgba(AtomContributionValues[i]))+\"\\n\")\n",
    "    \n",
    "    return m, AtomContributionValues\n",
    "\n",
    "def Make_CrystalMakerFile(elements, xyz, AtomContributionValues, m, saveResults, threshold):\n",
    "    # Read bonds and colors of all atoms\n",
    "    bonding = []\n",
    "    with open(\"/Users/dimitrygrebenyuk/Documents/GitHub/ML-MotEx/util/Bonding.txt\", 'r') as fi:\n",
    "        for line in fi.readlines():\n",
    "            sep_line=line.strip('{}\\n\\r ').split()\n",
    "            bonding.append(sep_line)\n",
    "    bonding = np.array(bonding)\n",
    "    \n",
    "    # Output a crystalmaker file to visualize the results\n",
    "    CrystalMaker = open(saveResults+'_CrystalMaker.cmtx', 'w')\n",
    "\n",
    "    CrystalMaker.write(\"MOLE  CrystalMaker molecule format\\n\")\n",
    "    CrystalMaker.write(\"TITL  Molecule\\n\\n\")\n",
    "    CrystalMaker.write(\"! Model type\\n\")\n",
    "    CrystalMaker.write(\"MODL  1\\n\\n\")\n",
    "\n",
    "    CrystalMaker.write(\"! Depth fading settings\\n\")\n",
    "    CrystalMaker.write(\"DCUE  1.000000 0.212899 0.704686\\n\\n\")\n",
    "\n",
    "    CrystalMaker.write(\"! Colour definitions:\\n\")\n",
    "    CrystalMaker.write(\"TYPE\\n\")\n",
    "\n",
    "    # Assign colors to all the atoms\n",
    "    for iter, element in enumerate(elements):\n",
    "        if iter < NumW:\n",
    "            #CrystalMaker.write(element + str(iter+1) + \" 1.32 \")\n",
    "            CrystalMaker.write(element + str(iter+1) + \" \" + bonding[np.where(bonding == element)[0][0], 1] + \" \")\n",
    "            rgb1 = m.to_rgba(AtomContributionValues[iter])[:-1][0]\n",
    "            rgb2 = m.to_rgba(AtomContributionValues[iter])[:-1][1]\n",
    "            rgb3 = m.to_rgba(AtomContributionValues[iter])[:-1][2]\n",
    "            CrystalMaker.write(str(rgb1) + \" \" + str(rgb2) + \" \" + str(rgb3))\n",
    "            CrystalMaker.write(\"\\n\")\n",
    "        else:\n",
    "            #CrystalMaker.write(element + str(iter+1) + \" 0.66 \")\n",
    "            CrystalMaker.write(element + str(iter+1) + \" \" + bonding[np.where(bonding == element)[0][0], 1] + \" \")\n",
    "            rgb1 = int(float(bonding[np.where(bonding == element)[0][0], 2])*255) #mpl.colors.to_rgb(\"#FF0000\")[0]\n",
    "            rgb2 = int(float(bonding[np.where(bonding == element)[0][0], 3])*255) #mpl.colors.to_rgb(\"#FF0000\")[1]\n",
    "            rgb3 = int(float(bonding[np.where(bonding == element)[0][0], 4])*255) #mpl.colors.to_rgb(\"#FF0000\")[2]\n",
    "            CrystalMaker.write(str(rgb1) + \" \" + str(rgb2) + \" \" + str(rgb3))\n",
    "            CrystalMaker.write(\"\\n\")\n",
    "    \n",
    "    CrystalMaker.write(\"\\n\")\n",
    "    CrystalMaker.write(\"! Atoms list\\n\")\n",
    "    CrystalMaker.write(\"! Bond Specifications\\n\")\n",
    "    \n",
    "    # Assign bonds between the atoms\n",
    "    for iter, element in enumerate(elements):\n",
    "        if iter < NumW:\n",
    "            NI_elements = np.delete(np.unique(elements), np.where(np.unique(elements) == element)[0])\n",
    "            for NI_element in NI_elements:\n",
    "                CrystalMaker.write(\"BMAX \" + element + \" \" + str(NI_element) + \"  \" + str(threshold))\n",
    "                CrystalMaker.write(\"\\n\")\n",
    "    \n",
    "    CrystalMaker.write(\"\\n\")\n",
    "    CrystalMaker.write(\"! Atoms list\\n\")\n",
    "    CrystalMaker.write(\"ATOM\\n\")\n",
    "    \n",
    "    # Assign coordinates to the atoms\n",
    "    for iter, element in enumerate(elements):\n",
    "        if iter < NumW:\n",
    "            CrystalMaker.write(element + \" \" + element + str(iter+1) + \" \" + str(xyz[iter][0]) + \" \" + str(xyz[iter][1]) + \" \" + str(xyz[iter][2]) + \"\\n\")\n",
    "        else:\n",
    "            CrystalMaker.write(element + \" \" + element + str(iter+1) + \" \" + str(xyz[iter][0]) + \" \" + str(xyz[iter][1]) + \" \" + str(xyz[iter][2]) + \"\\n\")\n",
    "\n",
    "    CrystalMaker.close()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def Make_VestaFile(elements, xyz, AtomContributionValues, m, saveResults, threshold):\n",
    "    # Read bonds and colors of all atoms\n",
    "    bonding = []\n",
    "    with open(\"/Users/dimitrygrebenyuk/Documents/GitHub/ML-MotEx/util/Bonding.txt\", 'r') as fi:\n",
    "        for line in fi.readlines():\n",
    "            sep_line=line.strip('{}\\n\\r ').split()\n",
    "            bonding.append(sep_line)\n",
    "    bonding = np.array(bonding)\n",
    "\n",
    "    # Output a Vesta file to visualize the results\n",
    "    Vesta = open(saveResults+'_Vesta.vesta', 'w')\n",
    "\n",
    "    Vesta.write(\"#VESTA_FORMAT_VERSION 3.5.4\\n\\n\\n\")\n",
    "    Vesta.write(\"MOLECULE\\n\\n\")\n",
    "    Vesta.write(\"Title\\n\")\n",
    "    Vesta.write(\"XYZ file\\n\\n\")\n",
    "\n",
    "    Vesta.write(\"STRUC\\n\")\n",
    "    # Assign coordinates to the atoms\n",
    "    for iter, element in enumerate(elements):\n",
    "        Vesta.write(str(iter+1) + \" \" + element + \" \" + element + str(iter+1) + \" 1.0000 \" + str(xyz[iter][0]) + \" \" + str(xyz[iter][1]) + \" \" + str(xyz[iter][2]) + \"1\" + \" -\" + \"\\n\")\n",
    "        Vesta.write(\"0 0 0 0\\n\")\n",
    "    Vesta.write(\"  0 0 0 0 0 0 0\\n\")\n",
    "\n",
    "    Vesta.write(\"SBOND\\n\")\n",
    "    # Assign bonds between the atoms\n",
    "    unique_elements = np.unique(elements)\n",
    "    for iter, element1 in enumerate(unique_elements):\n",
    "      for iter, element2 in enumerate(unique_elements):\n",
    "        if not element1 == element2:\n",
    "          Vesta.write(str(iter+1) + \" \" + element1 + \" \" + element2 + \" 0.0 \" + str(threshold) + \" 0 1 1 0 1 0.25 2 127 127 127\\n\")\n",
    "          Vesta.write(\"0 0 0 0\\n\")\n",
    "    \n",
    "    Vesta.write(\"SITET\\n\")\n",
    "    # Assign colors to all the atoms\n",
    "    for iter, element in enumerate(elements):\n",
    "        if iter < NumW:\n",
    "            rgb1 = int(m.to_rgba(AtomContributionValues[iter])[:-1][0]*255)\n",
    "            rgb2 = int(m.to_rgba(AtomContributionValues[iter])[:-1][1]*255)\n",
    "            rgb3 = int(m.to_rgba(AtomContributionValues[iter])[:-1][2]*255)\n",
    "            Vesta.write(str(iter+1) + \" \" + element + str(iter+1) + \" \" + bonding[np.where(bonding == element)[0][0], 1] + \" \" + str(rgb1) + \" \" + str(rgb2) + \" \" + str(rgb3) + \" \" + str(rgb1) + \" \" + str(rgb2) + \" \" + str(rgb3) + \" 204 0\\n\")\n",
    "        else:\n",
    "            rgb1 = int(float(bonding[np.where(bonding == element)[0][0], 2])*255)\n",
    "            rgb2 = int(float(bonding[np.where(bonding == element)[0][0], 3])*255)\n",
    "            rgb3 = int(float(bonding[np.where(bonding == element)[0][0], 4])*255)\n",
    "            Vesta.write(str(iter+1) + \" \" + element + str(iter+1) + \" \" + bonding[np.where(bonding == element)[0][0], 1] + \" \" + str(rgb1) + \" \" + str(rgb2) + \" \" + str(rgb3) + \" \" + str(rgb1) + \" \" + str(rgb2) + \" \" + str(rgb3) + \" 204 0\\n\")\n",
    "    Vesta.write(\"0 0 0 0 0 0\\n\")\n",
    "    \n",
    "    Vesta.write(\"ATOMT\\n\")\n",
    "    done_deal_atoms = []\n",
    "    for iter, element in enumerate(elements):\n",
    "      if element not in done_deal_atoms:\n",
    "        rgb1 = int(float(bonding[np.where(bonding == element)[0][0], 2])*255)\n",
    "        rgb2 = int(float(bonding[np.where(bonding == element)[0][0], 3])*255)\n",
    "        rgb3 = int(float(bonding[np.where(bonding == element)[0][0], 4])*255)\n",
    "        Vesta.write(str(iter+1) + \" \" + element + \" \" + bonding[np.where(bonding == element)[0][0], 1] + \" \" + str(rgb1) + \" \" + str(rgb2) + \" \" + str(rgb3) + \" \" + str(rgb1) + \" \" + str(rgb2) + \" \" + str(rgb3) + \" 204\\n\")\n",
    "        done_deal_atoms.append(element)\n",
    "    Vesta.write(\"0 0 0 0 0 0\\n\")\n",
    "\n",
    "    Vesta.close()\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHuN2OsMtypj"
   },
   "source": [
    "# Step 1: Produce a catalogue of structure motifs\n",
    "### First define the starting model, how large the structure catalogue has to be, number of iterable atoms and threshold between iterable atoms and non-iterable atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "5RyeCXx4ty0w",
    "outputId": "6d4895bc-7435-4674-92cb-da37ee82b413"
   },
   "outputs": [],
   "source": [
    "print (\"Upload a XYZ file of your starting model\")\n",
    "starting_model = 'th24.xyz' # Name of the starting model file\n",
    "Number_of_structures = 10000 # Number of structures made to the structure catalogue\n",
    "allowed_atoms = [\"Th\"] # The atoms that should be permuted, can be multiple atoms\n",
    "threshold = 3.0 # Threshold for W - O bond\n",
    "print(starting_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWo9QoNYt3aM"
   },
   "source": [
    "### Produce a catalogue of structure motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCmqJJXit3iA",
    "outputId": "e7336e8a-8925-41c5-ace9-128f3574c336"
   },
   "outputs": [],
   "source": [
    "NumW = format_XYZ(starting_model, allowed_atoms) # Format the starting model and calculate then number of atoms that should be permuted in the starting model\n",
    "structure_catalogue = structure_catalogue_maker(Number_of_structures, Number_of_atoms=NumW, lower_atom_number=0, higher_atom_number=NumW)\n",
    "print (\"We show the first 10 structures in the catalogue:\")\n",
    "structure_catalogue[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeOR90xmuJBK"
   },
   "source": [
    "# Step 2: Fit all of the structures from the catalogue of structure motifs to the dataset\n",
    "### First define the experimental data path and the path you want the structure catalogue with fits to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "cmeF4HTvuJIc",
    "outputId": "ef4e0dd4-f7a8-43a0-b9ef-ce203ac85584"
   },
   "outputs": [],
   "source": [
    "print (\"Upload a PDF file in gr format\")\n",
    "StemName = 'Th22_02_spot1_0001_0000_summed_bsub_tmean_gs2' # Upload PDF from local computer\n",
    "Experimental_Data = StemName + \".gr\" # Name of the experimental file\n",
    "saveFits = StemName + \".txt\" # Name of the saved fits file\n",
    "atom_ph, Qmin, Qmax, Qdamp, rmin, rmax = \"Th\", 0.5, 21, 0.02, 1, 15\n",
    "print(Experimental_Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2EnAVYRuOdC"
   },
   "source": [
    "### Produce organized structure catalogue with Rwp values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5x9IHiDuOlu",
    "outputId": "594894af-1f2b-4b22-a5d2-c151a56e5bc1"
   },
   "outputs": [],
   "source": [
    "Result = fitting_multiprocess(structure_catalogue, atom_ph, Qmin, Qmax, Qdamp, rmin, rmax, SaveName=saveFits, cores=8)\n",
    "print (\"The best fitting structure is:\")\n",
    "Result[np.argmin(Result[:,1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = fitting(structure_catalogue, atom_ph, Qmin, Qmax, Qdamp, rmin, rmax, plot=1, index=7600)\n",
    "#print (\"The best fitting structure is:\")\n",
    "#Result[np.argmin(Result[:,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXVWN7v00UIf"
   },
   "source": [
    "# Step 3: Train a XGBoost Regressor to predict the Rwp value based on the structure catalogue\n",
    "### Set range for model optimization. \n",
    "\n",
    "It can take a few minutes to train the XGBoost model. \n",
    "N_iter gives a measure of how rounds the model should try to optimize. Set to a low number for a less accurate but fast convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k6za1V2Q0UQ0",
    "outputId": "a9cae0e7-e8de-48ee-bf8d-9b0cca211300"
   },
   "outputs": [],
   "source": [
    "Min_LR = 0.1 # Minimum learning rate for the XGBoost algorithm\n",
    "Max_LR = 0.8 # Maximum learning rate for the XGBoost algorithm\n",
    "Min_Dep = 2 # Minimum max depth for the XGBoost algorithm\n",
    "Max_Dep = 10 # Maximum max depth for the XGBoost algorithm\n",
    "n_iter = 50 # Number of iterations for the bayesian optimization model\n",
    "\n",
    "# Import dataset\n",
    "StemName = 'Th22_02_spot1_0001_0000_summed_bsub_tmean_gs2' \n",
    "saveFits = StemName + \".txt\" # Name of the saved fits file\n",
    "X_train, y_train, X_val, y_val = Import_Dataset(saveFits)\n",
    "\n",
    "# Make a dictionary of parameters we should optimize for the ML algorithm\n",
    "parameters_BayesianOptimization = {\"learning_rate\": (Min_LR, Max_LR), #0.1 is default - Boosting learning rate\n",
    "                                   \"max_depth\": (Min_Dep, Max_Dep)}  #3 is default - Maximum tree depth for base learners.\n",
    "\n",
    "# Finding the best parameters for the ML model\n",
    "BayesianOptimization_func = optimize_DecisionTree(X_train, y_train, parameters_BayesianOptimization, n_iter=n_iter)\n",
    "print(\"Best parameters were: \", BayesianOptimization_func.max)\n",
    "\n",
    "\n",
    "# Get best parameters for the ML model and make max depth to an integer\n",
    "params = BayesianOptimization_func.max['params']\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "\n",
    "#  Define model, train and validate the model and save model\n",
    "model, store = train_w_earlyStop(X_train, y_train, n_jobs=1, seed=0, base_score=0.5, n_estimators=100, gamma=0, min_child_weight=1, early_stop=50, **params)\n",
    "Validate_XGBoost(model, X_val, y_val)\n",
    "model.save_model(StemName + \"_XGB_model.dat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6kRsee70YeD"
   },
   "source": [
    "# Step 4: Calculate atom contribution values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WY9eQsMn0YmY",
    "outputId": "3884f64f-7d57-419c-fd4c-c157b1135073"
   },
   "outputs": [],
   "source": [
    "# Calculate SHAP values and save to savePATH.\n",
    "explainer, shap_values = shap_essential_figure(model, X_train, \"./\")\n",
    "\n",
    "# Calculate atom contribution values\n",
    "m, AtomContributionValues = calculate_atomContributionValue(shap_values, X_train, \"./\")\n",
    "\n",
    "# Output a CrystalMaker and VESTA file\n",
    "elements, xyz = Load_starting_model(starting_model)\n",
    "Make_CrystalMakerFile(elements, xyz, AtomContributionValues, m, StemName, threshold)\n",
    "Make_VestaFile(elements, xyz, AtomContributionValues, m, StemName, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaTiV4jdhxDP"
   },
   "source": [
    "# Cite\n",
    "If you use ML-MotEx, please consider citing our paper. Thanks in advance!\n",
    "```\n",
    "@article{anker2022ML-MotEx,\n",
    "   title={Extracting Structural Motifs from Pair Distribution Function Data of Nanostructures using Explainable Machine Learning},\n",
    "   author={Andy S. Anker, Emil T. S. Kjær, Mikkel Juelsholt, Troels Lindahl Christiansen, Susanne Linn Skjærvø, Mads Ry Vogel Jørgensen, Innokenty Kantor, Daniel R. Sørensen, Simon J. L. Billinge, Raghavendra Selvan, Kirsten M. Ø. Jensen},\n",
    "   journal = {npj Computational Materials},\n",
    "   volume = {8},\n",
    "   number = {1},\n",
    "   pages = {213},\n",
    "   ISSN = {2057-3960},\n",
    "   DOI = {10.1038/s41524-022-00896-3},\n",
    "   url = {https://doi.org/10.1038/s41524-022-00896-3},\n",
    "   year = {2022}}\n",
    "```\n",
    "\n",
    "# LICENSE\n",
    "This project is licensed under the Apache License Version 2.0, January 2004 - see the LICENSE file at https://github.com/AndySAnker/ML-MotEx/blob/main/LICENSE for details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def catalogue_to_arrays(catalogue, number_of_best_structures):\n",
    "    '''This function reads the prepared catalogue of structures and returns\n",
    "    only desired number of best structures based on Rw value of the PDF fits.'''\n",
    "    with open(catalogue, 'r') as f:\n",
    "        strings = f.readlines()\n",
    "    filtered_strings = [s for s in strings if float(s.split()[1]) <= 0.95]\n",
    "    sorted_strings = sorted(filtered_strings, key=lambda x: float(x.split()[1]))\n",
    "    arrays = []\n",
    "    for s in sorted_strings:\n",
    "        # Split the string into a list of numbers\n",
    "        numbers = list(map(float, s.split()))\n",
    "        arrays.append(np.array(numbers))\n",
    "    return arrays[:number_of_best_structures]\n",
    "\n",
    "\n",
    "def modify_xyz_files(arrays, xyz_filename, new_dir):\n",
    "    '''This file takes the selected best structures and the initial xyz file with the starting model\n",
    "    and creates a series of '''\n",
    "    # Create the new directory if it doesn't exist\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "\n",
    "    # Open the input xyz file\n",
    "    with open(xyz_filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Get the number of atoms from the xyz file\n",
    "    num_atoms = int(lines[0])\n",
    "\n",
    "    # Iterate over the arrays\n",
    "    for array in arrays:\n",
    "        # Create a list to store the modified xyz lines\n",
    "        new_lines = []\n",
    "\n",
    "        # Append the header to the new lines list\n",
    "        new_lines.append(f\"{num_atoms}\\n\")\n",
    "        new_lines.append(lines[1])\n",
    "\n",
    "        # Iterate over the atoms in the xyz file\n",
    "        for i, line in enumerate(lines[2:]):\n",
    "            # If the index is less than the length of the array, process the atom according to the array value\n",
    "            if i < len(array):\n",
    "                if array[i] == 1:\n",
    "                    new_lines.append(line)\n",
    "            # If the index is greater than or equal to the length of the array, retain the atom\n",
    "            else:\n",
    "                new_lines.append(line)\n",
    "\n",
    "        # Compute the new xyz filename using the first two values of the initial array\n",
    "        new_filename = \"{:.2f}_{:.0f}.xyz\".format(array[1], array[0])\n",
    "\n",
    "        # Join the new directory and the new filename\n",
    "        new_path = os.path.join(new_dir, new_filename)\n",
    "\n",
    "        # Write the modified xyz file\n",
    "        with open(new_path, 'w') as f:\n",
    "            f.writelines(new_lines)\n",
    "\n",
    "def filter_atoms(path, new_directory, threshold_distance):\n",
    "    # Create new directory if it doesn't exist\n",
    "    if not os.path.exists(new_directory):\n",
    "        os.makedirs(new_directory)\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        # Prepare new filename and write the new file\n",
    "        new_filename = f\"{path.split('/')[-1]}\"\n",
    "        new_file = os.path.join(new_directory, new_filename)\n",
    "        with open(new_file, 'w') as new_f:\n",
    "            # write the header\n",
    "            new_f.write(lines[0])\n",
    "            new_f.write(lines[1])\n",
    "            # Create a list to store the Th coordinates\n",
    "            Th_coordinates = []\n",
    "            for line in lines[2:]:\n",
    "                fields = line.split()\n",
    "                if fields and fields[0] == \"Th\":\n",
    "                    new_f.write(line)\n",
    "                    Th_coordinates.append([float(fields[1]), float(fields[2]), float(fields[3])])\n",
    "                elif fields and fields[0] == \"O\":\n",
    "                    O_coordinate = [float(fields[1]), float(fields[2]), float(fields[3])]\n",
    "                    for Th_coordinate in Th_coordinates:\n",
    "                        distance = np.linalg.norm(np.array(Th_coordinate) - np.array(O_coordinate))\n",
    "                        if distance < threshold_distance:\n",
    "                            new_f.write(line)\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = catalogue_to_arrays('3000_from_th24_1423.txt', 5)\n",
    "modify_xyz_files(arrays, 'th24.xyz', '/Users/dimitrygrebenyuk/Documents/GitHub/ML-MotEx/catalogue_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Users/dimitrygrebenyuk/Documents/GitHub/ML-MotEx/catalogue_best'\n",
    "new_directory = '/Users/dimitrygrebenyuk/Documents/GitHub/ML-MotEx/catalogue_best/filtered'\n",
    "threshold_distance = 3.0\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".xyz\"):\n",
    "        path = os.path.join(directory, filename)\n",
    "        filter_atoms(path, new_directory, threshold_distance)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPmgMbi+ZQ0lXZJehr2Hs2H",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ML-MotEx-Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8a6fbbec4c542ceefa5ff5b41ec7754eef0dcfc01d745c483db9abc301bcdb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
